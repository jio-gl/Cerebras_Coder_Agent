\begin{thebibliography}{1}

\bibitem{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago,
  et~al.
\newblock Competition-level code generation with alphacode.
\newblock {\em Science}, 378(6624):1092--1097, 2022.

\bibitem{wang2021codet5}
Yue Wang, Weishi Wang, Shafiq Joty, and Steven~C.H. Hoi.
\newblock Codet5: Identifier-aware unified pre-trained encoder-decoder models
  for code understanding and generation.
\newblock {\em arXiv preprint arXiv:2109.00859}, 2021.

\end{thebibliography}
