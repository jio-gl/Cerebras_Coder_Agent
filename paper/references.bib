@misc{github2021copilot,
  title={GitHub Copilot: Your AI pair programmer},
  author={{GitHub}},
  year={2021},
  url={https://github.com/features/copilot},
  note={Accessed: 2024-01-15}
}

@article{wang2021codet5,
  title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven C.H.},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}

@article{chen2021evaluating,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{nijkamp2022codegen,
  title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{li2022competition,
  title={Competition-level code generation with AlphaCode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@misc{cerebras2024inference,
  title={Cerebras Inference: Ultra-Fast AI Model Serving},
  author={{Cerebras Systems}},
  year={2024},
  url={https://cerebras.net/inference/},
  note={Accessed: 2024-01-15}
}

@article{qwen2024,
  title={Qwen Technical Report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2024}
}

@inproceedings{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{fried2022incoder,
  title={InCoder: A Generative Model for Code Infilling and Synthesis},
  author={Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2204.05999},
  year={2022}
}

@article{roziere2023code,
  title={Code Llama: Open Foundation Models for Code},
  author={Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@misc{openrouter2024,
  title={OpenRouter: Universal API for LLMs},
  author={{OpenRouter}},
  year={2024},
  url={https://openrouter.ai/},
  note={Accessed: 2024-01-15}
}

@article{zhang2023unixcoder,
  title={UniXcoder: Unified Cross-Modal Pre-training for Code Representation},
  author={Zhang, Daoguang and Chen, Yaliang and Zhao, Bolin and Liao, Jing and Qian, Long and Lu, Shuai and Pan, Weizhu and Wang, Nan and Zhou, Ming and Guo, Daxin},
  journal={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  year={2022}
}

@inproceedings{feng2020codebert,
  title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Qin, Ming and Liu, Bing and Jiang, Daxin},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={1536--1547},
  year={2020}
} 